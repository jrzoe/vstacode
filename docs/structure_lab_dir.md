# Structuring you lab directory

Properly structing your lab's directory, and project subfolders therein, makes collaboration and data management much easier.

## Storing data within your lab partition

To begin, a critical piece is storing your lab's data effectively. ALL of your large data files generated by experiments that serve as input for downstream analyses should be stored in a dedicated data directory, completely separate from any project directories you may be coding in. For example, if your lab generates a lot of sequencing data, the fastq files and important derivatives (e.g. those made by a Snakemake/Nextflow pipeline) used for downstream analysis would fall into this category. 

To make this concrete and help explain why I think this is important, here's an example root structure for your lab's shared partition, with the separation of `data` and `projects` directories being the most important aspect:

```
/path/to/lab_partition/
├── containers/           # place to store container images (e.g. singularity)
├── data/                 # all raw and processed data files 
│   ├── cell_line_a_seq/  # fastqs, bams, etc from a specific experiment
│   ├── cell_line_b_seq/
│   └── mouse_line_1_seq/
├── home/                 # personal home directories for lab members
├── projects/           # where you will store your downstream analysis projects
│   ├── splicing_cell_line_a/ # project analyzing splicing in cell line A
│   ├── diff_exp_cell_lines_ab/ # project analyzing diff exp between cell lines A and B
│   └── diff_exp_mouse_line_1/ # project analyzing diff exp in mouse line 1
└── public_datasets/   # publicly available datasets you download
```

So why does separating input data from downstream projects matter?

1. You will often find that some downstream analyses may use data from multiple experiments or sources, or data from one experiment can be used for many downstream analysis projects. In the tree above, let's say we had one project analyzing splicing in cell line A (`splicing_cell_line_a`), and another analyzing differential expression between cell lines A and B (`diff_exp_cell_lines_ab`). If raw sequencing data was stored in the `projects` directory with analyses, there's no easy way to decide which directory should contain the original data for cell line A. As a result, someone who joins the lab after you may not have any idea where the raw input data lives; in this toy example this seems unlikely, but when you have a large number of project directories, data can be very difficult to track down, *especially* if directory names aren't as informative as they are above. 

2. You can give extra care to your critical, non-replaceable data files. Downstream results, including plots, tables, and processed data files, can always be regenerated from the original data files and your code. Your code will be stored remotely on GitHub, but your data files won't be. By storing all your data files in a single, dedicated location, you can set up stricter backup protocols for that directory to ensure data integrity. Furthermore, you can be sure to be more careful with file system operations (e.g. deleting files) within that directory.

3. When you find yourself short on storage space, you know the one directory with all big ticket items in terms of size. Rather than search through many project directories to find large files, you can simply check the `data` directory to see experimental data files that aren't used anymore and can be archived on a long-term storage solution (e.g. glacier storage on AWS).

With data files stored in a dedicated location, you can now set up your project directories for collaborative coding. Any input data files needed for a project should be symlinked from the `data` directory into the project directory (more on this below). This way, you don't have many copies of the same data file eating up your storage, and everyone working on the project can be sure they are using the same input data files. You should also do this for any publicly available datasets you download and use in your analyses (e.g. TCGA, SRA downloads), which can be stored separately in the `public_datasets` directory. 

## Project directory structure

Now, for setting up a project directory. It's helpful to use a consistent directory structure across all projects. This nice tree structure was taken from [The Good Research Code Handbook](https://goodresearch.dev/setup):

```
project-name/
├── data/               # Raw data files (not tracked in git)
├── docs/               # Documentation
├── results/            # Output files (not tracked in git)
├── scripts/            # Code scripts, Jupyter notebooks, etc
├── src/                # Source code modules
├── tests/              # Tests for your code
├── environment.yml     # Environment specification file
└── README.md           # Project overview
```

As mentioned above, the `data` directory will largely be a collection of symlinks to the actual data. When writing code stored in `scripts`, **ALL of your paths should be relative paths, never absolute paths.** When collaborating with other on the same compute cluster, this isn't strictly necessary, but it's good practice for when 1) you aren't on the same compute platform as your colleagues, or 2) you want to share your code publicly and allow others to run it (necessary for scientists!). With all of your data files symlinked into the `data` directory, you can use relative paths to access them easily.

Here is an example `.gitignore` file to ensure you are tracking only code files:

```
# ignore all files by default
*
# don't ignore directories, to allow for recursion into them
!*/
# track only code (can add others as needed)
!*.py
!*.ipynb
!*.[Rr]
!*.[Rr]md
!*.qmd
!*.sh
!*.md
# include gitignores in repo
!.gitignore
```